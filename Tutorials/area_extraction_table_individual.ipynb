{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pyproj import CRS\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, box, LineString\n",
    "import folium\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import re\n",
    "import psutil\n",
    "import sys\n",
    "import regionmask\n",
    "import cf_xarray\n",
    "from pyproj import CRS\n",
    "import dask.array as da\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a table with area extraction data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l2_flags', 'chl_re_gons']\n",
      "['l2_flags', 'chl_re_gons']\n",
      "['chl_re_gons', 'l2_flags']\n",
      "\n",
      "chl_re_gons\n",
      "\n",
      "P:\\11209243-eo\\Window_extraction\\INPUT\\L2W_V2\\L2W\\S2A_MSI_2015_07_16_10_50_24_merged_westerschelde_L2W.nc\n",
      "Mean Value: 4.5580034\n",
      "Mean Value: 5.7823076\n",
      "Mean Value: 6.202788\n",
      "Mean Value: 8.163575\n",
      "Mean Value: 7.471553\n",
      "Mean Value: 6.783728\n",
      "Mean Value: 7.2073584\n",
      "Mean Value: 7.6196404\n",
      "Mean Value: 8.107584\n",
      "P:\\11209243-eo\\Window_extraction\\INPUT\\L2W_V2\\L2W\\S2A_MSI_2023_08_23_10_56_47_merged_westerschelde_L2W.nc\n",
      "Mean Value: 6.923731\n",
      "Mean Value: 10.881442\n",
      "Mean Value: 9.95523\n",
      "Mean Value: 9.2135\n",
      "Mean Value: 8.180277\n",
      "Mean Value: 9.897763\n",
      "Mean Value: 8.231522\n",
      "Mean Value: 7.764074\n",
      "Mean Value: 8.970564\n",
      "\n",
      "l2_flags\n",
      "\n",
      "P:\\11209243-eo\\Window_extraction\\INPUT\\L2W_V2\\L2W\\S2A_MSI_2015_07_16_10_50_24_merged_westerschelde_L2W.nc\n",
      "Mean Value: 0.8367055887333987\n",
      "Mean Value: 0.7427937915742794\n",
      "Mean Value: 0.5029823493609251\n",
      "Mean Value: 0.6500514933058702\n",
      "Mean Value: 0.9200646326394829\n",
      "Mean Value: 0.9007881316643487\n",
      "Mean Value: 0.5688193085453359\n",
      "Mean Value: 1.1062992125984252\n",
      "Mean Value: 0.9689199689199689\n",
      "P:\\11209243-eo\\Window_extraction\\INPUT\\L2W_V2\\L2W\\S2A_MSI_2023_08_23_10_56_47_merged_westerschelde_L2W.nc\n",
      "Mean Value: 0.2919155004902398\n",
      "Mean Value: 0.7844296624784429\n",
      "Mean Value: 0.25130858186244676\n",
      "Mean Value: 0.4105046343975283\n",
      "Mean Value: 0.5588822355289421\n",
      "Mean Value: 0.41492814093648583\n",
      "Mean Value: 0.39921722113502933\n",
      "Mean Value: 0.35958005249343833\n",
      "Mean Value: 0.7824397824397824\n"
     ]
    }
   ],
   "source": [
    "def print_memory():\n",
    "    # Get memory information\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    print(f\"Memory Usage: {memory_info.percent:.2f}%\")\n",
    "\n",
    "\n",
    "# Define a function to extract the date from the path. Data abount the year\n",
    "def extract_date(path):\n",
    "    match = re.search(r'(\\d{4}_\\d{2}_\\d{2})', path)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return \"0000_00_00\"  # Return a default date if no date is found\n",
    "\n",
    "# Define function to extract pixel values\n",
    "def get_window(x_coord, y_coord, stationID, dataset):\n",
    "    time_series   = dataset.sel(x=x_coord, y=y_coord, method='nearest')\n",
    "    y_coord_exact = time_series['y'].values.tolist()\n",
    "    x_coord_exact = time_series['x'].values.tolist()\n",
    "    y_index = np.where(dataset['y'] == y_coord_exact)[0].tolist()[0]\n",
    "    x_index = np.where(dataset['x'] == x_coord_exact)[0].tolist()[0]\n",
    "    central_lon  = x_index\n",
    "    central_lat  = y_index\n",
    "\n",
    "    lat_start, lat_end = central_lat - 1, central_lat + 2\n",
    "    lon_start, lon_end = central_lon - 1, central_lon + 2\n",
    "\n",
    "    window_values = dataset.isel(y=slice(lat_start, lat_end), x=slice(lon_start, lon_end))\n",
    "    window_values = window_values.expand_dims(station=[stationID])\n",
    "    \n",
    "    return window_values\n",
    "\n",
    "# folder_path_l2w_data = r'P:\\11209243-eo\\Window_extraction\\INPUT\\L2W'\n",
    "folder_path_l2w_data = r'P:\\11209243-eo\\Window_extraction\\INPUT\\L2W_V2\\L2W'\n",
    "folder_path_shp      = r'P:\\11209243-eo\\Window_extraction\\OUTPUT\\polygons_NEOZ.geojson'\n",
    "excel_output_path = 'P:/11209243-eo/Window_extraction/OUTPUT/' \n",
    "\n",
    "# List all files in the folder\n",
    "files_in_folder = os.listdir(folder_path_l2w_data )\n",
    "filtered_files = [file for file in files_in_folder if file.endswith(\"L2W.nc\")]\n",
    "sorted_files = sorted(filtered_files, key=extract_date)\n",
    "\n",
    "# Remove variables that are not used\n",
    "variables_to_remove = ['transverse_mercator', 'x', 'y', 'lon', 'lat']\n",
    "\n",
    "# Get variables names in a list. This component is hardcoded and is reading one S2A and one S2B image.\n",
    "ds_a = xr.open_dataset(os.path.join(folder_path_l2w_data, sorted_files[0])).drop_vars(variables_to_remove)\n",
    "ds_b = xr.open_dataset(os.path.join(folder_path_l2w_data, sorted_files[-1])).drop_vars(variables_to_remove)\n",
    "\n",
    "variable_names_A = list(ds_a.variables)\n",
    "variable_names_B = list(ds_b.variables)\n",
    "variable_names = sorted(list(set(variable_names_A + variable_names_B)), key=str.lower)\n",
    "\n",
    "print(variable_names_A)\n",
    "print(variable_names_B)\n",
    "print(variable_names)\n",
    "\n",
    "# Reduce dataset for testing \n",
    "sorted_files = sorted_files[:]\n",
    "variable_names =variable_names[:]\n",
    "\n",
    "polygons_gdf = gpd.read_file(folder_path_shp)\n",
    "polygons_gdf = polygons_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "for variable_name in variable_names:\n",
    "\n",
    "    print('\\n'+ variable_name+'\\n')\n",
    "\n",
    "    df_list = []\n",
    "    ds_list = []\n",
    "    stations_list = []\n",
    "    statistics = []\n",
    "\n",
    "    for file in sorted_files:\n",
    "        path = os.path.join(folder_path_l2w_data, file)\n",
    "        print(path)\n",
    "\n",
    "        # Open dataset\n",
    "        # dataset = xr.open_dataset(path)\n",
    "        dataset = xr.open_dataset(path, chunks={'x': 100, 'y': 100})\n",
    "\n",
    "        crs_wkt = dataset.transverse_mercator.attrs['crs_wkt']\n",
    "        crs = CRS.from_string(crs_wkt)\n",
    "        dataset.rio.write_crs(crs.to_epsg(), inplace=True)\n",
    "        dataset = dataset.rio.reproject('EPSG:4326')\n",
    "\n",
    "        variables_to_remove = ['lon', 'lat']\n",
    "        dataset =  dataset .drop_vars(variables_to_remove)\n",
    "        dataset = dataset.rename({'x': 'lon', 'y': 'lat'})\n",
    "        dataset   = dataset.reset_coords(['transverse_mercator'])\n",
    "      \n",
    "        #Assign time component as a variable\n",
    "        time_series = [datetime.fromisoformat(dataset.attrs.get(\"isodate\"))]  \n",
    "        ds = xr.concat([dataset[variable_name]], dim=xr.DataArray(time_series, coords={\"time\": time_series}, dims=[\"time\"]))\n",
    "\n",
    "        for index, row in polygons_gdf.iterrows():\n",
    "            ksa_aoi  = polygons_gdf[polygons_gdf.Group == row['Group']]\n",
    "\n",
    "            # Get coord bounds with buffer for clipping\n",
    "            ksa_lat = [float(ksa_aoi.total_bounds[1]), float(ksa_aoi.total_bounds[3])]\n",
    "            ksa_lon = [float(ksa_aoi.total_bounds[0]), float(ksa_aoi.total_bounds[2])]\n",
    "            lat_multiplier = abs(ksa_lat[1] - ksa_lat[0])\n",
    "            lon_multiplier = abs(ksa_lon[1] - ksa_lon[0])\n",
    "\n",
    "            min_lon, max_lon = ksa_lon[0] - (0.1*lon_multiplier), ksa_lon[1] + (0.1*lon_multiplier)\n",
    "            min_lat, max_lat = ksa_lat[0] - (0.1*lat_multiplier), ksa_lat[1] + (0.1*lat_multiplier)\n",
    "\n",
    "            # Generate mask\n",
    "            ksa_mask = regionmask.mask_geopandas(\n",
    "                ksa_aoi, \n",
    "                ds.lon, \n",
    "                ds.lat)\n",
    "\n",
    "            # Clip and mask\n",
    "            ds_clip = ds.where((ds.lat <= max_lat) & (ds.lat >= min_lat)\\\n",
    "                            & (ds.lon <= max_lon) & (ds.lon >= min_lon), drop=True)\n",
    "            ds_masked = ds_clip.where(~ksa_mask.isnull())\n",
    "\n",
    "            # Calculate the mean value for the selected area\n",
    "            mean_value = ds_masked.mean().values\n",
    "            median_value = ds_masked.median().values\n",
    "            min_value = ds_masked.min().values\n",
    "            max_value = ds_masked.max().values\n",
    "            std_value = ds_masked.std().values\n",
    "            q25 = ds_masked.quantile(0.25).values\n",
    "            q75 = ds_masked.quantile(0.75).values\n",
    "            iqr_value = q75 - q25\n",
    "            count_value = ds_masked.count().values\n",
    "\n",
    "            print(\"Mean Value:\", mean_value)\n",
    "\n",
    "            # Plot the masked DataArray\n",
    "            # plt.figure(figsize=(10, 8))\n",
    "            # group = str(row['Group'])\n",
    "            # plt.title(f'Masked {variable_name} with Polygon: {group}')\n",
    "            # ds_masked.isel(time=0).plot(cmap='viridis', robust=True)\n",
    "            # polygons_gdf.plot(ax=plt.gca(), edgecolor='red')\n",
    "            # plt.show()\n",
    "\n",
    "            statistics.append((time_series, row['Group'], mean_value, median_value, min_value, max_value, std_value, q25 , q75, iqr_value, count_value))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Time': [coord[0][0]for coord in statistics],\n",
    "        'Group': [coord[1] for coord in statistics],\n",
    "        variable_name +'_mean': [coord[2].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_median': [coord[3].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_min': [coord[4].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_max': [coord[5].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_std': [coord[6].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_q25': [coord[7].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_q75': [coord[8].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_iqr': [coord[9].round(decimals=4) for coord in statistics],\n",
    "        variable_name +'_count': [coord[10] for coord in statistics],\n",
    "    })\n",
    "\n",
    "    df['ID'] =  df['Time'].dt.strftime('%Y_%m_%d') + '_' + df['Group'].round().astype(int).astype(str) \n",
    "\n",
    "    # df.set_index('Time', inplace=True)\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "    # Create a single table \n",
    "    merged_df = df_list[0].iloc[:,2:]  \n",
    "\n",
    "    for df in df_list[1:]:\n",
    "        merged_df = pd.merge(merged_df, df.iloc[:,2:], on='ID', how='outer')\n",
    "\n",
    "    split_columns = merged_df['ID'].str.split('_', expand=True)\n",
    "    merged_df.insert(0, 'Year', split_columns[0].astype(str))\n",
    "    merged_df.insert(1, 'Month', split_columns[1].astype(str))\n",
    "    merged_df.insert(2, 'Day', split_columns[2].astype(str))\n",
    "    merged_df.insert(3, 'Group', split_columns[3].astype(str))\n",
    "\n",
    "    merged_df.insert(4, 'Date', pd.to_datetime(merged_df[['Year', 'Month', 'Day']], errors='coerce').dt.date)\n",
    "\n",
    "    merged_df = merged_df.replace('nan', np.nan)\n",
    "    merged_df.set_index('ID', inplace=True)\n",
    "\n",
    "    merged_df.to_excel(excel_output_path + f'area_extraction_table_{variable_name }.xlsx',na_rep='nan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydromt-fiat-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
